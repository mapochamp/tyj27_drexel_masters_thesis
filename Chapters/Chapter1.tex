
\chapter{Background} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Deep Learning Architectures}

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Perceptron}

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{CNN}

%-----------------------------------
%	SUBSECTION 3
%-----------------------------------

\subsection{RNN}


%-----------------------------------
%	SUBSECTION 4
%-----------------------------------

\subsection{LSTM}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Accelerators and Scratchpad Memory}
Deep learning accelerators (DLA) are hardware made specifically to accelerate
common operations needed to run inference or training on DL models. 

\subsection{Existing Scratchpad Memory Management Schemes}

\section{Deep Learning Frameworks}
\subsection{Introduction}
In order to create to run a DL model, a programmer must describe the
architecture, its input and output layers, and the inputs that will be passed
to the model. Beyond these properties, the implementation and challenges
associated with deploying such a model on a DLA or distributed system is
handled by deep learning frameworks. Deep learning frameworks abstract the
implementation and execution details from the programmer such that the
programmer may focus on the model creation only. The abstraction of such
details forces the trade-off of performance and generality as the frameworks
must optimize the same model for runtime and memory efficiency on a diverse set
of available hardware that all have different memory heiarchies, number of PEs,
and supported operations. Frameworks must be able to decipher model
descriptions in high level languages such as Python, apply hardware agnostic
optimizations, and map operations to a specific hardware create an executable
that will run the model to completion device.

Conventional compilers (eg. GCC) typically take a high level language down to
an intermediate representation (IR) to then machine code that can run on a
device.  The front end is responsible for what parses and lexes the high level
code, the middle end is responsible for lowering it to IR and running
optimization passes on the IR, and the backend lowers the IR further into
machine code. Similarly, DL frameworks can be broken down into a front-end,
middle-end, and backend. However, there are major differences in the role of
each partition and how they are performed.  Deep learning frameworks deal with
a small set of operations that are highly parallelizable but with
variable-sized data \cite{nGraph}. Some operations include: sigmoid, matrix
multiply, vectorsum, L1 Norm, element wise multiply, and dropout among others
\cite{cntk}. Further, a programmer for within a deep learning framework will
work with a domain specific language (DSL) such as Tensorflow
\cite{tensorflow}. Because DSLs only concern with the description of deep
learning networks, the runtime execution, memory, and device management is
managed by the framework. A framework that enables the deployment of a deep
learning model onto hardware from only a high level language description is
called an end to end framework.

%Computational graphs
\subsection{Computational Graphs}
A common interface used by most DL frameworks is the computational graph
\cite{tensorflow} \cite{cntk} that represents a network of operations needed to
run a DL model. Each DL model can be broken down into a series of discrete
computational operations that can be applied to an input to describe a given
model. A graph made up of vertices and edges where the vertices represent an
operation and the edges represent tensors that flow from one operation to the
next can be used to fully describe a DL model. Computational graphs are
directed acyclic graphs (DAG).  Each vertex can have between zero and two
inputs and zero to one outputs. So for each model, each node in each layer can
be broken down into an operation that maps to a node in the graph. An operation
is a particular computation that must be ran with a set of inputs and outputs.
The implementation of the operation is defined by a kernel. A kernel is
specific to a hardware architecture and are mapped to each operation by the
backend. For example a matrix multiply operation is an essential operation
that reoccurs frequently in computation graphs; the implementation of a matrix
multiply will depend on how many processing elements (PE) the DLA has and its
memory hierarchy. A tensor is a multi-dimensional array that can take on many
types (eg., floating point, int8) and are the inputs and outputs of operations.
Tensors are what are passed along edges of the graph. The dataflow represented
by a graph can be used to infer dependencies and possible optimizations for the
middle-end to apply.

% TODO: algorthim of a one hiddle layer sigmoid like in CNTK

% TODO: figure of DAG -> operation -> kernel code snippet

% TODO: figure of Matrix operations -> DAG -> kernel code

% TODO: figure of NN -> DAG

%IR
\subsection{IR}
Intermediate representation (IR) is an alternative representation of DL models
for some frameworks such as \cite{DLVM} \cite{nGraph} \cite{ONNX}. For each
framework that uses IR, the framework typically uses its own IR, however
there are tools such as ONNX that attempt to standardize the IR used in
frameworks. 

IR in DL frameworks are a language that support linear algebra instructions and
other general purpose instructions (eg., branch, conditional- branch). DL
models are lowered into the framework's IR and a DAG of control flow
\cite{ngraph} can be created where the vertices are operations and edges are
data. This is similar to a computational graph, however, the expressivness and
modularity that IR provides may lead to more optimization oppurtunities
\cite{DLVM}. In a computation graph, each kernel can be tuned to maximize its
performance in isolation. Outside of manual kernel turning, performance gains
come from the reordering of schedules and changing the access pattern of
tensors.  In IR based frameworks, the same types of optimizations can be done
to each instruction that maps to a kernel. However, the kernels in IR are not
picked from a set of pre-made kernels in a supported backend, but rather code
generated into LLVM IR or another lower level IR based on the framework. This
allows static analysis to better fuse redundant operations and kenerls together
in a more efficient way than sequentially ordering kernels as done in
computational graph based frameworks. Further, different IRs may come with
a wider range of operations that may be supported than the set of operations in
a computational graph that allow the compiler and code generator to manipulate
the DAG in more ways.

Once a DL model is lowered into IR and its optimization passes are done, the IR
is either passed into a bridge code generator or a more supported IR such as
LLVM IR. A bridge will transform the framework IR into a format that is
compatible wit h a particular backend. There must be a bridge made for every
backend. Otherwise, in the case of LLVM based IR transformations, only
backends that support LLVM IR are able to be used to generate a final binary.

% TODO: DL arch -> IR -> IR CFG

%Front end
\subsection{Front End}
The front end of frameworks are responsible for breaking down the high level
descriptions of a DL model, typically in Python or C++ \cite{tensorflow}, and
converting them into computation graphs or IR. These lower level abstractions
allow the framework to more easily apply optimizations for memory and runtime
efficiency.

%middle end
\subsection{Middle End}
Most optimizations that occur and the engineering focus of end to end
frameworks lie in the middle end. Middle ends must have a diverse set of
backens in mind and be hardware agnostic. In order to achieve this, an IR or
computational graph representation of a DL model is received from the front
end.  Once a constructed dataflow graph in either operator or IR form is
passed, several passes of static analysis and optimization occur \cite{TVM}
\cite{DLVM} \cite{tensorflow}. Once a completely optimized dataflow graph is
created, it must then be formatted to be compatible to a particular backend.
This can either mean serializing a computational graph into a one that a
backend can read or applying a transform to the optimized IR to generate code
that is compatible with a backend.  Backends are vendor specific compilers that
deal with the device level details for kernel implementations and tuning.
Back-ends will support a subset of formats (ie either serialized computational
graphs or a specific IR) and to that end, there must be significant engineering
effort to support a backend for a given middlend. Consequently, most frameworks
only support a handful or backends such as Nvidia's CUDA backend or BLAS for
CPUs. For supported backends, the middlend must be sure to create a final
computational graph or set of IR modules that do not contain operations or
instructions that are not supported by the predefined kernels in the vendor
provided library of a backend \cite{TVM}.

Optimizations in the middle end can be separated into IR specific and
computational graph specific optimizations. IR optimizations 

%backend
\subsection{Back End}
"Backendsâ€™ implementations are encapsulated so that the same computation can
execute on multiple backends, such as CPUs and GPUs." - (ngraph)

"Leveraging Specific Hardware Features and Abstrac-
tions. DL accelerators introduce optimized tensor com-
pute primitives [1, 12, 21], while GPUs and CPUs con-
tinuously improve their processing elements. This poses
a significant challenge in generating optimized code for
a given operator description. The inputs to hardware in-
structions are multi-dimensional, with fixed or variable
lengths; they dictate different data layouts; and they have
special requirements for memory hierarchy. The system
must effectively exploit these complex primitives to ben-
efit from acceleration. Further, accelerator designs also
commonly favor leaner control [21] and offload most
scheduling complexity to the compiler stack. For spe-
cialized accelerators, the system now needs to gener-
ate code that explicitly controls pipeline dependencies to
hide memory access latency â€“ a job that hardware per-
forms for CPUs and GPUs" - TVM




%Graph level optimizations

