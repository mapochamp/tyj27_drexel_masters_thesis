
\chapter{Background} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Deep Learning Architectures}

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------
\subsection{Perceptron}

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------
\subsection{CNN}

%-----------------------------------
%	SUBSECTION 3
%-----------------------------------

\subsection{RNN}


%-----------------------------------
%	SUBSECTION 4
%-----------------------------------

\subsection{LSTM}

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------
\section{Accelerators and Scratchpad Memory}
Deep learning accelerators (DLA) are hardware made specifically to accelerate
common operations needed to run inference or training on DL models. 

\subsection{Existing Scratchpad Memory Management Schemes}

\section{Deep Learning Frameworks}
\subsection{Introduction}
In order to create to run a DL model, a programmer must describe the architecture, its input and output layers, and the inputs that will be passed to the model. Beyond these
properties, the implementation and challenges associated with deploying such a model
on a DLA or distributed system is handled by deep learning frameworks. Deep learning
frameworks abstract the implementation and execution details from the programmer such
that the programmer may focus on the model creation only. The abstraction of such
details forces the trade-off of performance and generality as the frameworks
must optimize the same model for runtime and memory efficiency on a diverse set
of available hardware. Frameworks must be able to decipher model descriptions
in high level languages such as Python, apply hardware agnostic optimizations,
and create an executable that will run the model to completion on the target
device.

Deep learning frameworks are different from conventional compilers (eg. GCC) by
having to deal with a small set of operations that are highly parallelizable
but with variable-sized data \cite{nGraph}. Some operations include: sigmoid,
matrix multiply, vectorsum, L1 Norm, element wise multiply, and dropout among
others \cite{cntk}. Further, conventional compilers typically take a high level
language down to an intermediate representation (IR) to then machine code that
can run on a device.  The front end is responsible for what parses and lexes
the high level code, the middle end is responsible for lowering it to IR and
running optimization passes on the IR, and the back-end lowers the IR further
into machine code. Similarly, DL frameworks can be broken down into a
front-end, middle-end, and back-end.

%Computational graphs
\subsection{Computational Graphs}
A common interface used by most DL frameworks is the computational graph
\cite{tensorflow} \cite{cntk} that represents a network of operations needed to
run a DL model. Each DL model can be broken down into a series of discrete
computational operations that can be applied to an input to describe a given
model. A graph made up of vertices and edges where the vertices represent an
operation and the edges represent tensors that flow from one operation to the
next can be used to fully describe a DL model. Computational graphs are
directed acyclic graphs (DAG).  Each vertex can have between zero and two
inputs and zero to one outputs. So for each model, each node in each layer can
be broken down into an operation that maps to a node in the graph. An operation
is a particular computation that must be ran with a set of inputs and outputs.
The implementation of the operation is defined by a kernel. A kernel is
specific to a hardware architecture and are mapped to each operation by the
back-end. For example a matrix multiply operation is an essential operation
that reoccurs frequently in computation graphs; the implementation of a matrix
multiply will depend on how many processing elements (PE) the DLA has and its
memory hierarchy. A tensor is a multi-dimensional array that can take on many
types (eg., floating point, int8) and are the inputs and outputs of operations.
Tensors are what are passed along edges of the graph. The dataflow represented
by a graph can be used to infer dependencies and possible optimizations for the
middle-end to apply.

% TODO: algorthim of a one hiddle layer sigmoid like in CNTK

% TODO: figure of DAG -> operation -> kernel code snippet

% TODO: figure of Matrix operations -> DAG -> kernel code

% TODO: figure of NN -> DAG

%IR
\subsection{IR}
Intermediate representation (IR) is an alternative representation of DL models
for some frameworks such as \cite{DLVM} \cite{nGraph} \cite{ONNX}. For each
framework that uses IR, the framework typically uses its own IR, however
there are tools such as ONNX that attempt to standardize the IR used in
frameworks. 

IR in DL frameworks are a language that support linear algebra instructions and
other general purpose instructions (eg., branch, conditional- branch). DL
models are lowered into the framework's IR and a DAG of control flow
\cite{ngraph} can be created where the vertices are operations and edges are
data. This is similar to a computational graph, however, the expressivness and
modularity that IR provides may lead to more optimization oppurtunities
\cite{DLVM}. In a computation graph, each kernel can be tuned to maximize its
performance in isolation. Outside of manual kernel turning, performance gains
come from the reordering of schedules and changing the access pattern of
tensors.  In IR based frameworks, the same types of optimizations can be done
to each instruction that maps to a kernel. However, the kernels in IR are not
picked from a set of pre-made kernels in a supported back-end, but rather code
generated into LLVM IR or another lower level IR based on the framework. This
allows static analysis to better fuse redundant operations and kenerls together
in a more efficient way than sequentially ordering kernels as done in
computational graph based frameworks. Further, different IRs may come with
a wider range of operations that may be supported than the set of operations in
a computational graph that allow the compiler and code generator to manipulate
the DAG in more ways.

Once a DL model is lowered into IR and its optimization passes are done, the IR
is either passed into a bridge code generator or a more supported IR such as
LLVM IR. A bridge will transform the framework IR into a format that is
compatible wit h a particular back-end. There must be a bridge made for every
back-end. Otherwise, in the case of LLVM based IR transformations, only
back-ends that support LLVM IR are able to be used to generate a final binary.

% TODO: DL arch -> IR -> IR CFG

%Front end
\subsection{Front End}
The front end of frameworks are responsible for breaking down the high level
descriptions of a DL model, typically in Python or C++ \cite{tensorflow}, and
converting them into computation graphs or IR. These lower level abstractions
allow the framework to more easily apply optimizations for memory and runtime
efficiency.

%middle end
\subsection{Middle End}

%backend
\subsection{Back End}
"Backends’ implemen-
tations are encapsulated so that the same computation can execute
on multiple backends, such as CPUs and GPUs." - (ngraph)



%Graph level optimizations

%IR level optimizations
Optimizations include domain- specific optimizations, such as algebra
simplification, linear algebra fusion, matrix multiplication reordering, and AD
checkpointing, and traditional compiler optimizations. - DLVM 

Since DLVM IR is aware of mathematical operators such as tanh and power, the
algebra simplifi- cation pass can find and simplify certain mathematical
operations that are expensive or redundant.  For example, x2 can be simplified
to x x ( stands for element-wise multiplication), and x0 can be simplified to
constant 1. Matrix multiplication reordering is another classic optimization
that minimizes the number of sub-operations in a chain of matrix
multiplications with different dimensionality, based on matrix multiplication’s
associativity.  Since the DLVM optimizer is aware of linear algebra operations
with static dimensionality, maxi- mizing the performance by fusing verbose
linear operations into a single matrix multiplication is beneficial as well.
For example, it is very common to encounter expressions of the form Wx + b.
When unoptimized, the matrix multiplication and the addition will be
parallelized separately. Since launching compute kernels separately can be
expensive, DLVM performs linear algebra fusion, which transforms subexpressions
involving both matrix multiplication and element-wise operations into a single
matrix multiplication instruction on padded tensors. Besides the simple pattern
like an addition of matrix multiplication and a vector, we can apply the same
approach to a polynomial of multiple matrix multiplications, turning the
polynomial into a single matrix multiplication. For example, in a simple
recurrent neural network (RNN), each cell of the recurrence is a feed forward
neural network that takes two inputs: xt, the input local to the current
timestep, and ht, the hidden state carried along the recurrence. The linear
algebra fusion pass can simplify operations in ht = f (Wxt−1 +Uht−1 +b) from
two matrix multiplications and two additions into a single matrix
multiplication. A more aggres- sive, interprocedural version of linear algebra
fusion can optimize parameter passing and memory allocation, so that the entire
concatenated matrix can be created and passed around in the first place without
reallocation - DLVM

