-----------------------------------------------------------------------------
|                                                                           |
|                                                                           |
|                       Accelerators and scratchpad memory                  |
|                                                                           |
|                                                                           |
-----------------------------------------------------------------------------

The on-chip SPMs are abstractions for the on-chip buffers of DNN
accelerators. They are fast but with size limitation, and are visible to operator
programmers and compiler. They usually play the role of caching partial input data
and temporary computational results. While the off-chip DRAMs are large but slow.
They are used for the storage of large quantity DNN parameters and input/output
data required by operators. The communications between the on-chip SPMs and the
off-chip DRAMs are accomplished through explicit DMA load and store
instruction - toplib

For most reconfigurable DNN accelerators, there are at least three levels of memory
hierarchy: off-chip memory (DRAM), on-chip scratchpad memory (SPM) and
registers. Unlike the caches of CPUs, which are managed by the hardware
automatically and are invisible to programmers, the on-chip buffers of DNN
accelerators are usually managed by programmers explicitly due to performance and
power concerns. To simplify the programmability of DNN accelerators, we hide the
hardware execution details and expose the performance-critical parts to
programmers - toplib

These SIMD instructions usually have the requirements of specific memory
alignment and data layout. For example, for the vector addition instruction of
Cambricon-ACC, the memory address of two source vectors’ must be multiple of 32
bytes. For Cambricon-ACC, the convolution, pooling and matrix multiplication
intrinsic primitives have specific data layout requirements. The data layout of
their input filters must follow the NHWC (N: batch size, H: height, W: width,
C: channel) layout requirement. Primitives load and store denote the DMA
communications between the on-chip SPMs and off-chip DRAM - toplib



- There is a growing demand to deploy smart applications to a wide spectrum of
devices, rang- ing from cloud servers to self-driving cars and embed- ded
devices. Mapping DL workloads to these devices is complicated by the diversity
of hardware characteristics, including embedded CPUs, GPUs, FPGAs, and ASICs
(e.g., the TPU [21]). These hardware targets diverge in terms of memory
organization, compute functional units, (TVM)

- FPGAs, asics, neuromorphic, systolic array, TPU, GPU. 
- operation types  (primitives)
	- Operations include matrix multiplication, convolution, DMA loads and stores, and arithmetic instructions \cite{tensorflow}.

- accelerators don't do shit if you dont program it to be utilization maxxing (onsram)
- low precision arhitemetic, architures to exploint parallelism, software managed scratchpad memory hierarchy (onsram)

- thousands of PEs and custom ISAs.\ specialized memeory hiercy structures and inrterconnect topologies (deeptools)
-  They are hard to program and you need framworks to ease the programmibility of all these accels and its fucking hard (deeptools)
- 

Leveraging Specific Hardware Features and Abstrac-
tions. DL accelerators introduce optimized tensor com-
pute primitives [1, 12, 21], while GPUs and CPUs con-
tinuously improve their processing elements. This poses
a significant challenge in generating optimized code for
a given operator description. The inputs to hardware in-
structions are multi-dimensional, with fixed or variable
lengths; they dictate different data layouts; and they have
special requirements for memory hierarchy. The system
must effectively exploit these complex primitives to ben-
efit from acceleration. Further, accelerator designs also
commonly favor leaner control [21] and offload most
scheduling complexity to the compiler stack. For spe-
cialized accelerators, the system now needs to gener-
ate code that explicitly controls pipeline dependencies to
hide memory access latency – a job that hardware per-
forms for CPUs and GPUs - TVM

So basically: accels are all special and are made for DL so they have special
memory and PEs made for processing large tensors in a certain way. but based
on the memory layout and PE layout, the way you want to feed tensors differ.
And because they all support different types of operations, different
operations can be fused and others can't and others just don't exist or certain
special operations exist for some but not for others. so programming them is hard
and its up to the compiler to figure out what is the optimal set of operations
to use. The level of acceleration you get from these accelerators is dependant
on how well they are programmed. Scheduling, memory control, and pipeline dependencies
are now all up to the programmer, something CPUs and GPUs handle for you within the
hardware.

-----------------------------------------------------------------------------
|                                                                           |
|                                                                           |
|                       deep learning compilers and frameworks              |
|                                                                           |
|                                                                           |
-----------------------------------------------------------------------------
- what are end to end frameworks?
	- automagically execute DNN graph descriptions on AI accelerator
	backends (deeptools)
	- contain a runtime scheduler to successfully execute each graph node
	in a goob way on the target device (deeptools)
	- allow for programmers to focus on machine leraning and not have to
	worry about hardware specific programming (deeptools)
	- A key difference between compilers for languages
	like C++ and compilers for deep learning frameworks is that with
	deep learning, the data being operated on is large and variable-sized,
	but highly amenable to parallelization (ngraph)
	- 

- can be broken down into graph creation, graph optimiziation/IR lowering +
  optimization, kernel serach, context creation, engine/runtime creation +
  scheduling, compiling to host + device binary

- types of end to end frameworks?

- COPY NGRAPH / ONNX / DEEPTOOLS IR SHPEEL


- Current DL frameworks, such as TensorFlow, MXNet, Caffe, and PyTorch, rely on a
computational graph in- termediate representation to implement optimizations,
e.g., auto differentiation and dynamic memory man- agement [3, 4, 9].
Graph-level optimizations, however, are often too high-level to handle hardware
back-end- specific operator-level transformations. Most of these frameworks
focus on a narrow class of server-class GPU devices and delegate
target-specific optimizations to highly engineered and vendor-specific operator
li- braries. These operator-level libraries require significant manual tuning
and hence are too specialized and opaque to be easily ported across hardware
devices. Providing support in various DL frameworks for diverse hardware
back-ends presently requires significant engineering ef- fort. Even for
supported back-ends, frameworks must make the difficult choice between: (1)
avoiding graph optimizations that yield new operators not in the prede- fined
operator library, and (2) using unoptimized imple- mentations of these new
operators. (TVM)
	- What do they mean by graph level IR? the graph becomes IR? what do
	they mean by the backends? I need toreread this shit. can they
	elaborate please? what the fuck?
	- read 3, 4, 9

- computation types written on CNTK paper: negate, sigmoid, tanh, relu, log, vectorsum, l1 norm, scale, mat mul, element wise multiply, dropout ,etc.

- mxnet does a thing for BLAS and CUDA where there are premade kernels ready to
be deployed and called and so they just look at the graph, figure out what
operations can be fused, and call the sequence of kernels that are optimized
for memory/compute and pass the needed params and pointers. This is basically
what happens during runtime for other architectures too like ours. Either there
is a lookup table of kernels to call or the entire thing is built as a single
binary based on the backend. This is because there is still some host-device
interplay here and not everything is ran just on the device. if it is, then its
entirely up to the backend on how to manage it.

- there are mappings between operators and operator libraries to backends. There
are unoptimzied and optimized versions too.  (TVM)


- lets make sure we understand what actually gets passed to the backend. ie, is
the backend or middle end generating kernels? are passing a binary? are we
passing a graph? what about JIT versions?
- what if the backend doesn't suport the IR the framework makes? -> it doesn't get  supported as a backend for that framework
- how are we translating graph to kernel? we make graph of ops and we have
dedicated kenerals premade in the backend to match with the ops
- what happens to unsupported operations?

- all frameworks assume architectures have unified memory except for
distributed cases (mxnet) (onsram)

- recall: there has to be a corresponding backend to a middle end that supports everything that gets connected to it. otherwise it wouldnt be a framework. So either a backend supports multiple middle end outputs or the other way around. a middle end can create different outputs for backends -> BIG FUCKING CITATION NEEDED

- ok so like theres a few ways we can send something to the backend: an optimized control flow graph itself where each node is an operation that exists as a call to a kernel to the target device or an io_ctl call where we can just dma stuff into 

- ok and like the the programmer never actually creates a runtime or execution engine that schedules shit and does the loads and stores. they only write the model. so like al l of that is created and generated by the backend. so a runtime execution engine is created that does the scheduling, memory management, and kernel calls. so really the bakcend is is some super code generator and compiler. For example tensorRT:
	- build phase: "responsible for optimizing a model, and producing an ENGINE"
		- the builder searches for available kernels to call and build together 		to execute your graph.
		- builder eliminates dead coputations, folds constants (wtf?),
		and reorders and combines operations to run more efficiently on
		the GPU. this also means reducing and quantizing fpus
		- the engine creates a serialized plan which can be saved to
		disk or deserialized immedietly for use
	- runtime phase: deserialize a plan to create an engine and create an
	execution context from the engine. Then, repeatedly: populate input
	buffers for inference, call enqueue() on the execution context to run
	inference
		- the engine just means the execution context: the saved model,
		inputs, outputs, size of the inputs outputs, the network graph
		of operations etc

- Optimizations types: internode and intra node

- optimizations on coomputations graphs:
it implements
many graph-level optimizations, including: operator fu-
sion, which fuses multiple small operations together;
constant-folding, which pre-computes graph parts that
can be determined statically, saving execution costs; a
static memory planning pass, which pre-allocates mem-
ory to hold each intermediate tensor; and data layout
transformations, which transform internal data layouts
into back-end-friendly forms. We now discuss operator
fusion and the data layout transformation. (TVM)

Middle end note:
- read TVMs operator fusion, data layout transformation sections for optimziations




-----------------------------------------------------------------------------
|                                                                           |
|                                                                           |
|                                IR                                         |
|                                                                           |
|                                                                           |
-----------------------------------------------------------------------------
"An nGraph framework bridge acts as
a framework backend. Each nGraph backend has a transformer
that compiles or interprets the IR and provides an allocation and
execution API that the framework bridges use to implement the
framework’s API" - ngraph. 
if you have you're own IR then you have to create a bridge or something
to transform it into the backend. efforts like ONNX try to standardize the use
of MLIR since every framework wants to claim that their IR is easier to reason
about or more optimizable than the other. But in the end, you can't just have
some arbitrary IR be interpreted by a backend without your own translating
bridge like ngraph. the bridges are code generators and theres a code generator
per backend. so honeslty idk what the hypes is about bc IR and computational graphs
are literally the same shti since IR just creates a DAG dataflow graph anyway of
ops... like theres literally no difference. The same passes of optimzations
happen anyway and you still need a specific bridge/code generator per backend
simiarlly to the way you need to create a seperate kernel per operation for
computational graphs... which a vendor will typically provide to you anyway
otherwise it would be fucking useless lmofa.


ok ok ok. so rather than a computational graph, we create an IR of linear algebra
operations and we convert to LLVM IR. This is done using code generators
to generate code from a high level description into IR into backend comapatible IR - DLVM

you have a cpu code generator, DLA kernel code generator, and LLVM driver and
some kind of framework runtime/memeory manager - DLVM

differences in the dataflow graph of an IR based and computational graph is the 
granularity of the nodes and how expressive each node is. so like you can have a
generic mat mul node that has its own kernel in the computational graph version
but in the IR version that gets broken down further ig? I need better examples
to see if i actually understand this correctly. -> not really broken down
further but you get to have a larger range of operation mappings without
necessarily needing hand made kernels. so you can have operations like branch
and conditional branch and have a code generator take care of that for you so
that the optimzier can then figure out more things it can do rather than just
gluing a bunch of kernels together - DLVM

modelled after LLVM where you have basic blocks, module, functions, and
instructions. instructions are the lowest thing you can add.  The code
representation has a hierarchy of abstractions: module, function, basic block,
and instruction. An instruction is the minimal unit of code that operates on
values, which can be globals, function arguments or temporary virtual registers
produced by instructions. Each module contains a collection of type
definitions, global values and functions. Each function has a control flow
graph formed by basic blocks and control flow edges. Each basic block contains
an ordered list of instructions with data dependencies forming a directed
acyclic graph - DLVM

OK so DLVM converts DLVM IR into LLVM IR. and so basically, unless you create
bridges manually, you are only supporting whatever backends support LLVM IR.
similarly though, you're only supporting backends that support serialized
computation graphs when using computational graphs instead of IR.

Existing LLVM utilities are used to compile the generated LLVM IR to the final binary. - DLVM

%IR level optimizations

Optimizations include domain- specific optimizations, such as algebra
simplification, linear algebra fusion, matrix multiplication reordering, and AD
checkpointing, and traditional compiler optimizations. - DLVM 

Since DLVM IR is aware of mathematical operators such as tanh and power, the
algebra simplifi- cation pass can find and simplify certain mathematical
operations that are expensive or redundant.  For example, x2 can be simplified
to x x ( stands for element-wise multiplication), and x0 can be simplified to
constant 1. Matrix multiplication reordering is another classic optimization
that minimizes the number of sub-operations in a chain of matrix
multiplications with different dimensionality, based on matrix multiplication’s
associativity.  Since the DLVM optimizer is aware of linear algebra operations
with static dimensionality, maxi- mizing the performance by fusing verbose
linear operations into a single matrix multiplication is beneficial as well.
For example, it is very common to encounter expressions of the form Wx + b.
When unoptimized, the matrix multiplication and the addition will be
parallelized separately. Since launching compute kernels separately can be
expensive, DLVM performs linear algebra fusion, which transforms subexpressions
involving both matrix multiplication and element-wise operations into a single
matrix multiplication instruction on padded tensors. Besides the simple pattern
like an addition of matrix multiplication and a vector, we can apply the same
approach to a polynomial of multiple matrix multiplications, turning the
polynomial into a single matrix multiplication. For example, in a simple
recurrent neural network (RNN), each cell of the recurrence is a feed forward
neural network that takes two inputs: xt, the input local to the current
timestep, and ht, the hidden state carried along the recurrence. The linear
algebra fusion pass can simplify operations in ht = f (Wxt−1 +Uht−1 +b) from
two matrix multiplications and two additions into a single matrix
multiplication. A more aggres- sive, interprocedural version of linear algebra
fusion can optimize parameter passing and memory allocation, so that the entire
concatenated matrix can be created and passed around in the first place without
reallocation - DLVM


%Graph level optimizations

operator fu- sion, which fuses multiple small operations together;
constant-folding, which pre-computes graph parts that can be determined
statically, saving execution costs; a static memory planning pass, which
pre-allocates mem- ory to hold each intermediate tensor; and data layout
transformations, which transform internal data layouts into back-end-friendly
forms. We now discuss operator fusion and the data layout transformation - TVM

Operator fusion: combining multiple operators into a single operation. Why this is
helpful: different operators that are scheduled that can otherwise be fused may
be scheduled in an unoptimized way. Every operation requires an input to be loaded
into the accelerator and an output to be produced to be loaded back into main memory.
Without explicit caching, or in cases where caching may not be feasible due to
size constraints of the accelerator memory which would require tiling, this
incurs extra memory loading/storing costs that adds additional time to inference.
By fusing operators together, the cost of memory operations are removed. Further,
by fusing ceratain oepratoins together, operation code generation can be optimzied
as certain intermediate values may not have to be stored or other computations
may be able to be skipped based on the available transformations that can be
applied. The downside to this is that, fused operations have to either be code
generated where the performance of the kernel is dependant on the quality of
the code generator without manual tuning or vendor provided libraries must have
pre-defined fused operators that have been optimized available to be used. As the
number of operators and operations on a network grows, the number of possible operators
that can be fused aslo grows with it. This means that without significant manual
effort, code generation tools may create suboptimal kernels or vendor libraries may
run out of pre-defined fused operators for the optimzier to work with, forcing a 
suboptimal graph of unoptimally fused operators. The number of optimially fused
operators grows combinoatroially with the number of supported data layouts and data
types and accelertor intrincts are supported \cite{TVM}. No backend could possible
have every combination of these fused operators for every DL architecture that exists,
much less new operators that may be invented in teh future.

Datalayout transformation:
Different DLAs perform best on different sized and dimensions of tensors. Reformatting
data to be tiled or adding reshape operators to better shape the data in such a way
the DLA can exploit is one optimization to minimize inference time.


%BACKEND
"Backends’ implementations are encapsulated so that the same computation can
execute on multiple backends, such as CPUs and GPUs." - (ngraph)

"Leveraging Specific Hardware Features and Abstrac-
tions. DL accelerators introduce optimized tensor com-
pute primitives [1, 12, 21], while GPUs and CPUs con-
tinuously improve their processing elements. This poses
a significant challenge in generating optimized code for
a given operator description. The inputs to hardware in-
structions are multi-dimensional, with fixed or variable
lengths; they dictate different data layouts; and they have
special requirements for memory hierarchy. The system
must effectively exploit these complex primitives to ben-
efit from acceleration. Further, accelerator designs also
commonly favor leaner control [21] and offload most
scheduling complexity to the compiler stack. For spe-
cialized accelerators, the system now needs to gener-
ate code that explicitly controls pipeline dependencies to
hide memory access latency – a job that hardware per-
forms for CPUs and GPUs" - TVM

- ok and like the the programmer never actually creates a runtime or execution engine that schedules shit and does the loads and stores. they only write the model. so like al l of that is created and generated by the backend. so a runtime execution engine is created that does the scheduling, memory management, and kernel calls. so really the bakcend is is some super code generator and compiler. For example tensorRT:
	- build phase: "responsible for optimizing a model, and producing an ENGINE"
		- the builder searches for available kernels to call and build together 		to execute your graph.
		- builder eliminates dead coputations, folds constants (wtf?),
		and reorders and combines operations to run more efficiently on
		the GPU. this also means reducing and quantizing fpus
		- the engine creates a serialized plan which can be saved to
		disk or deserialized immedietly for use
	- runtime phase: deserialize a plan to create an engine and create an
	execution context from the engine. Then, repeatedly: populate input
	buffers for inference, call enqueue() on the execution context to run
	inference
		- the engine just means the execution context: the saved model,
		inputs, outputs, size of the inputs outputs, the network graph
		of operations etc

The backend of an end-to-end framework is the final compiler that produces
a binary to be executed. This involves taking the output of a middle end (ie.
serialized computational graph, IR) and mapping the operations to a kernel
or compiling the IR generated code into a kernel for the DLA. Kernels are then
statically analyzed to identify and apply optimizations such as loop unrolling
and loop reordering to minimize latency. At this point, no runtime execution
context, memory allocations, or scheduler has been created by the front or middle
end and is up to the compiler to generate. All optimizations and transforms
that have been applied have been only to the DL model description provided
by the programmer.

TensorRT includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for deep learning inference applications. The core of NVIDIA TensorRT is a C++ library that facilitates high-performance inference on NVIDIA GPUs. TensorRT takes a trained network, which consists of a network definition and a set of trained parameters, and produces a highly optimized runtime engine that performs inference for that network. Refer to the following TensorRT product documentation for more information. - TensorRT


just do stuff. neber stay idle. shkreli will fire u if you do.


For the sake of brevity, we omit the kernel explanation details. But for now, it
suffices to see that even this trivial MLP implementation involves many low-level
programming chores. For example, the vector-vector and matrix-vector instructions,
e.g., VAV and MMV, usually need special registers to store the memory address and
the size of input data. Programmers have to manually allocate these registers and
track the lifetime of each memory block, which is burdensome and error-prone.
Besides, these CISC style instructions usually have special address alignment
requirements, programmers have to manually check the address alignment of each
memory block. This kind of low-level coding is very typical during the development
of DNN operators. - toplib

^ include this into the backend kernel shpeel and talk about how its important
to do non kernel based optimziations to squeeze as much hardware agnostic
optimziatiosn as possible bc kernel based shits are hard af. Also include into the
backend kenrel shpeel by talkin about howhard it is to do code generation and have
good operater fusion and manual tuning so we need the backend to do this shit for us
if middle end did operator fusion. like explain that the middle end has to know that
certain operator fusions can happen based on the backed and its hte backends responsibilty
to konw how to create those fused oeprators by either probding vendor provided
predeinfed fused operators or using code generation liek this paper to create the kenrnesl
idk make things sound really complicated by talking about spceifics of hardware like this
paper so that we can talk more shit in our background that isn't relevant to the thesis


we should have a section on ILP probably or someshit. idk how much space to
give to this section. we should also talk about binpacking and time variant binapcking.
oh fuck i need to read up on this before talking to shack

The trick of
expression template is done by letting operations and functions return abstracted
operation objects that contain all necessary information to construct the expression
tree, instead of calculating the result themselves. The expression tree is only
evaluated when the actual need arises (i.e., lazy evaluation). Through the use of
template meta-programming, it is even possible to manipulate the expression tree at
compile time to apply algebraic transformations (enumeration based rewriting
rules). For example, we define fst as a notational shorthand for map(fst, x), where x
is a n-order tensor, marked as ½x0; x1; :::; x n1, i.e., an array of tuples. Function fst
yields a tensor which is composed of the first component of each tuple in x, i.e.,
fstðxÞ ¼ ½xð0Þ
0 ; xð0Þ
1 ; :::; xð0Þ
n1. Programmers wrote such an algorithmic expression
y ¼ fstðreduceðþ0; xÞÞ 1 , where reduce operator þ0 means apply a reduction
summation along the first dimension of the input tensor x. TOpLib will transform
this expression into y ¼ reduceðþ0; fstðxÞÞ 2 . Obviously, reduce operation þ0 is
more computation-intensive than the map operation fst. Compared with expression
1 , the fst operation in 2 can filter out input data of the reduce operation and
maintain the original semantics. This transformation can eliminate redundant
computations, and thereby reducing the overall cost - top lib


