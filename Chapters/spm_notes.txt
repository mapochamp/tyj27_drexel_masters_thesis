-----------------------------------------------------------------------------
|                                                                           |
|                                                                           |
|                       scratchpad memory outline                           |
|                                                                           |
|                                                                           |
-----------------------------------------------------------------------------

- what a spm is

- where spm's are used
	- embedded
	- DLA

- register allocation
	- graph coloring
	- spilling

- scratchpad memory management
	- graph coloring
		- graph coloring + memory coloring paper + toplib
	- heursitics
		- papers referenced in graph coloring paper
		- on sram
		- top lib
	- ILP
		- papers referenced in graph coloring paper + multicore w/e paper
	- virtual memory / MMU style

-----------------------------------------------------------------------------
|                                                                           |
|                                                                           |
|                       notes + scratch                                     |
|                                                                           |
|                                                                           |
-----------------------------------------------------------------------------
Toplib does graph coloring for all the tensors and i'm guessing so does onsram. how tf do you do this on
multiple spms?

toplib does a thing where the partition sizes of the spm is the max(same color tensors)
so its basically register allocation but with heterogenous register sizes.

I guess i can do this with register spilling on 3 spms

Greedy implementation: they don't specify how they color or build the graph but they do
tell you the entire algorthim if you figure that part out. all you know is that its greedy
coloring. ig i can find an implmentation online somehere..

-----------------------------------------------------------------------
-----------------------------------------------------------------------
----------------------------what is spm?-------------------------------
-----------------------------------------------------------------------
-----------------------------------------------------------------------

The advent of Deep Neural Networks (DNNs) and the growth of Artificial
Intelligence (AI) based applications and services has revitalized the
application landscape across the spectrum of computing devices, from mobile
phones and edge devices to data centers and the cloud. The success of DNNs can
be attributed in part to their large-scale structure (100s of layers with
millions of parameters), which imposes a high computational cost. This has
triggered a rapid evolution in computer architecture in recent years, from many
demonstrations of specialized accelerators for DNNs [6, 10, 14, 16, 19, 29, 36,
49, 59, 63, 76, 91, 94] to AI cores featured in many commercial products, such
as Google TPUs, NVIDIA Tensor Cores, Intel NNP [1, 2, 100], among others.
Computations in DNNs can be expressed using a small number (few 10s) of
primitives, which lends itself well to hardware acceleration. DNN accelerators
have constantly pushed the enve- lope of compute efficiency, achieving peak
processing capabilities from 100s of GOPS/W to >10 TOPS/W. As summarized by
Hennessy and Patterson in the 2019 Turing Award Lecture [40], the key hardware
techniques to boost compute efficiency include low-precision arithmetic,
architec- tures to exploit parallelism, and software-managed Scratchpad Memory
(SPM) hierarchy. How- ever, the key to the success of AI accelerators lies in
how they are programmed to sustain high utilization with little/no loss in
end-user productivity. The heterogeneity in computing character- istics across
layers and operations makes performance-aware programming a challenge. - ONSRAM

Scratchpad memory (SPM) [3] is a kind of fast on-chip memory managed by
software (a programmer or a compiler), while cache has to query the flag bit
which is managed by hardware to check cache misses or hits. Compared with the
hardware cache, SPM does not need to perform flag bit judgment and other tasks,
and has the advantages of low power consumption and fast access. SPM is
initially used in embedded systems to meet the real-time and time predictable
requirements of embedded systems [17]. Besides, the Scratchpad memory is also
extensively used in FPGAs and is also employed as application-specific caches
[33, 34]. The current heterogeneous many-core processors, such as Adapteva
Epiphany, Sunway TaihuLight, and IBM Cell, also use SPM to achieve better per-
formance and lower energy consumption. The characteristic of this type of
archi- tecture is that each accelerator core has its own SPM that can be
accessed at high speed but has limited space.  The SPM is connected to the
off-chip memory through a bus [26]. The accel- erator cores can only access the
data of off-chip memory directly by global load/ store instructions or direct
memory access (DMA) [11, 29]. The accelerator cores can communicate with each
other by a network-on-chip (NoC). However, pro- grammers need to explicitly
manage the data transfer between the SPM and the off-chip memory in the
application, which hinders program development - many core architectures

Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely
used in embedded systems. This article introduces a general-purpose compiler
approach, called memory coloring, to assign static data aggregates, such as
arrays and structs, in a program to an SPM. The novelty of this approach lies
in partitioning the SPM into a pseudo–register file (with interchangeable and
aliased registers), splitting the live ranges of data aggregates to create
potential data transfer statements between SPM and off-chip memory, and
finally, adapting an existing graph coloring algorithm for register allocation
to assign the data aggregates to the pseudo–register file. - graph coloring
paper

 First, SPMs are more energy efficient and cost efficient
than caches since they do not need complex tag-decoding logic. Second, in em-
bedded applications with regular data access patterns, an SPM can outperform
a cache memory, since software can better choreograph the data movements
between SPM and off-chip memory. Finally, such a software-managed strategy
guarantees better timing predictability, which is critical in hard real-time sys-
tems. Given these advantages, SPMs have been increasingly incorporated in
modern embedded systems - graph coloring paper

^ in the same vein, because DL workloads have deterministic schedules, we spms
are a good choice over caches when running statically compiled graphs. In the
case of eager mode, this is debatable.

For SPM-based systems, the programmer or compiler must schedule explicit
data transfers between SPM and off-chip memory. The effectiveness of such
an SPM management affects critically the performance and energy cost of an
application. In today’s industry, this task is largely accomplished manually.
The programmers often spend a lot of time on partitioning data and insert-
ing explicit data transfers required between SPM and off-chip memory. Such
a manual approach is often time consuming and error prone. Moreover, the
handcrafted code is not portable, since it is usually customized to a particular
architecture - graph coloring paper

-----------------------------------------------------------------------
-----------------------------------------------------------------------
-----------------------------------------------------------------------

To overcome these limitations, we propose a general-purpose compiler ap-
proach, called memory coloring, to determining the dynamic allocation and
deallocation of static data aggregates, such as global and stack-allocated
arrays and structs in a C-like program, so as to maximize the performance of
the pro- gram. Whenever we speak of arrays in this article, we mean both kinds
of data aggregates. An array whose size exceeds that of the SPM under
consideration cannot be placed entirely in the SPM. Such arrays can be tiled
into smaller “ar- rays” by means of loop tiling n the proposed approach, the
continuous space of an SPM is parti- tioned into a pseudo–register file (with
interchangeable and aliased registers).  The data aggregates are the register
candidates to be assigned to the SPM via a generalized graph coloring
allocator, which can be any traditional graph coloring allocator generalized as
described in Smith et al. [2004] to handle in- terchangeable and aliased
registers. Unlike scalars, data aggregates typically have longer live ranges,
so live range splitting may be used beneficially to split their live ranges
into smaller pieces. The splitting points are the places to insert all required
data transfers between SPM and off-chip memory. During the col- oring phase,
register coalescing is applied to reduce unnecessary data transfers that would
otherwise have been introduced into the final program - graph coloring paper


This component will partition the continuous space of an SPM into a pseudo–
register file (with interchangeable and aliased registers). This is the key to
allowing us to map the SPM allocation problem into the classic register alloca-
tion problem. The sizes of arrays in a program considered for SPM allocation
are aligned to a predefined constant value, ALIGN UNIT, in bytes. All arrays
with a common aligned size are clustered into a common equivalent class called
an array class.  For each array class of a particular size, the SPM is
partitioned into a register class such that each register in the class can hold
exactly one array of that size.  A detailed algorithm that formalizes one
partitioning scheme can be found in Li et al.

^ so basically they cluster all the arrays with the same size/alignemnt
together and then partition registers based on those sizes.

If i'm reading this correctly, they split array live ranges into just the used
portions. in our case
we use the entire thing lmao.



Existing methods: ILP, heurstic, color graphing


% register allocation
the relationship between run-time performance and effective use of a ma-
chine’s register set is well understood. In a compiler, the process of deciding
which values to keep in registers at each point in the generated code is called
register allocation - register allocation

popular techniques for performing register allocation are based on a graph
coloring paradigm. These allocators construct a graph representing the con-
straints that the allocator must preserve. Using graph techniques, they
discover a mapping from values in the procedure to registers in the target
machine; the mapping must observe the constraints.  - register allocation


% ILP
bro many core archtiecture is fucking useless. all it does is just figure out
an optimal granularity of data transfers. not an actual pinning schedule.
basically a glorified tiler.

verma ILP allocation is literally just extending the ILP model for register allocation
into the SPM by splitting SPM into registers as well.

