% Chapter Template

\chapter{Related Work} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Existing Memory Optimizations in Deep Learning Frameworks}
There exist many end to end deep learning frameworks that allow
high level DSL programmers to create models that will be optimized to run
on a given hardware. Because memory management is such a significant
factor in runtime performance, all frameworks employ their own specialized
optimization methods based on their unique model representation forms.

% IR based ones (ngraph and DLVM)
% simplify shit to remove unnessary datamovement by creating smaller or fused
% kernels so that you don't have to have to jam together a bunch of complex
% kernels to complete your dataflow. 
% graph based ones (smaug and TVM)

TVM by Chen, et al. 2018 is a graph based end to end framework. 
TVM aims to create a framework that targets
multiple backends and accelerator architectures while still applying
fine grained optimizations for both intra and inter operator computations.
The main problem Chen et al. solved was the problem of existing deep learning
compiler frameworks only applying high level graph optimizations that did
not allow for tuning in operator specific implementations and left that to
the backends. However, the existing frameworks only supported a small handful
of GPU and CPU based backends. Creating support for new hardware backends
requires significant manual effort and there was no automated solution that 
existed. TVM allows for multiple backends to be supported while also
applying high graph level and operator implementation level optimizations
within its framework. Optimizations include operator fusion, SPM management via
operator rescheduling, pipelined memory access, and machine learning (ml) based
cost modeling for code generation. Operator fusion is when kernels are fused
together into a single kernel to avoid the need to save intermediate results
to main memory. Operator rescheduling is the rescheduling of operators based
on data flow dependencies to maximize reuse by placing operators whose
inputs are outputs of another next to each other. Pipelined memory accesses are
when memory access instructions are executed without compute instructions having
been completed yet so that the next compute instruction does not need to wait
for the next memory instruction to complete. TVM also introduces an ML
based modeling method that estimates the performance cost of different
code generated and generates the lowest cost code to compile into the backed.

DLVM \cite{dlvm} (Wei et al. 2018) lowers a model into a domain specific IR
such that modern compiler optimizations could be performed on the operations
and simplified before LLVM IR is code generated for a backend to create
individual kernels. Deep learning compilers \cite{tensorflow} \cite{torch}
prior to this work would take a computational graph of operations and use off
the shelf vendor provided kernels to implement them. While the kernels
themselves are highly tuned, further optimizations could be made such as
algebra implication, kernel fusion, and other modern compiler techniques such
as dead code elimination and sub expression elimination could be applied to
further increase performance. By reducing the amount of operations needed to be
applied by analyzing which linear algebra operations could be simplified and
applying further parallelization optimizations once it is lowered into LLVM IR,
DLVM is able to minimize the amount of data transfer needed for kernels and how
much data is needed to run the kernels themselves.

After DLVM, Cyphers, Bansal, et al. 2018 at Intel created nGraph \cite{nGraph},
another IR based deep learning framework that takes an approach of using its
own IR. Instead of lowering a domain specific IR into LLVM after optimization
passes, nGraph takes the IR and passes it to a transformer, a code generator
that produces optimized backend code or code to be linked to a specific
backend's kernels. The goal was to create a more flexible IR that better supported
tensor based instructions and could be compiled in a more targeted fashion compared
to generic LLVM IR. Transformers also remove the dependency of backends having
to support LLVM.

% explicit focus on memory reuse (top lib, many core architecutre, onsram)
