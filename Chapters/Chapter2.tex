% Chapter Template

\chapter{Related Work} % Main chapter title

\label{Chapter2} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Existing Memory Optimizations in Deep Learning Frameworks}
There exist many end to end deep learning frameworks that allow
high level DSL programmers to create models that will be optimized to run
on a given hardware. Because memory management is such a significant
factor in runtime performance, all frameworks employ their own specialized
optimization methods based on their unique model representation forms.

% IR based ones (ngraph and DLVM)
% simplify shit to remove unnessary datamovement by creating smaller or fused
% kernels so that you don't have to have to jam together a bunch of complex
% kernels to complete your dataflow. 
% graph based ones (smaug and TVM)

TVM by Chen, et al. 2018 is a graph based end to end framework. 
TVM aims to create a framework that targets
multiple backends and accelerator architectures while still applying
fine grained optimizations for both intra and inter operator computations.
The main problem Chen et al. solved was the problem of existing deep learning
compiler frameworks only applying high level graph optimizations that did
not allow for tuning in operator specific implementations and left that to
the backends. However, the existing frameworks only supported a small handful
of GPU and CPU based backends. Creating support for new hardware backends
requires significant manual effort and there was no automated solution that 
existed. TVM allows for multiple backends to be supported while also
applying high graph level and operator implementation level optimizations
within its framework. Optimizations include operator fusion, SPM management via
operator rescheduling, pipelined memory access, and machine learning (ML) based
cost modeling for code generation. 
Operator rescheduling is the rescheduling of operators based
on data flow dependencies to maximize reuse by placing operators whose
inputs are outputs of another next to each other. Pipelined memory accesses are
when memory access instructions are executed without compute instructions having
been completed yet so that the next compute instruction does not need to wait
for the next memory instruction to complete. TVM also introduces an ML
based modeling method that estimates the performance cost of different
code generated and generates the lowest cost code to compile into the backed.
This means that the search space of parameters such as loop tiling size and 
loop unrolling factors are automated and decided based on the ML model.

DLVM \cite{DLVM} (Wei et al. 2018) lowers a model into a domain specific IR
such that modern compiler optimizations could be performed on the operations
and simplified before LLVM IR is code generated for a backend to create
individual kernels. Deep learning compilers \cite{tensorflow} \cite{torch}
prior to this work would take a computational graph of operations and use off
the shelf vendor provided kernels to implement them. While the kernels
themselves are highly tuned, further optimizations could be made such as
algebra implication, kernel fusion, and other modern compiler techniques such
as dead code elimination and sub expression elimination could be applied to
further increase performance. By reducing the amount of operations needed to be
applied by analyzing which linear algebra operations could be simplified and
applying further parallelization optimizations once it is lowered into LLVM IR,
DLVM is able to minimize the amount of data transfer needed for kernels and how
much data is needed to run the kernels themselves.

After DLVM, Cyphers, Bansal, et al. 2018 at Intel created nGraph \cite{nGraph},
another IR based deep learning framework that takes an approach of using its
own IR. Instead of lowering a domain specific IR into LLVM after optimization
passes, nGraph takes the IR and passes it to a transformer, a code generator
that produces optimized backend code or code to be linked to a specific
backend's kernels. The goal was to create a more flexible IR that better supported
tensor based instructions and could be compiled in a more targeted fashion compared
to generic LLVM IR. Transformers also remove the dependency of backends having
to support LLVM.

% Toplib
Li et al. 2020 introduced TOpLib \cite{toplib}: a compiler-assisted operator
template library for DNN accelerators that focused on automated kernel
generation with optimized memory management using graph coloring. TOpLib builds
on the graph coloring solution to the register allocation problem applied to
SPM management for embedded devices but for DNNs. The key difference is that in
non-DL workloads, allocations are targeted for single dimensional arrays whose
sizes are known statically. Meanwhile, for DL workloads, tensors are high
dimensional, variable sized,  and can often be larger than the size of the SPM
itself.  This requires an approach to dynamically partition the SPM into
variable sized psuedo-registers rather than equal sizing. The general concept
that they apply is as follows. A static walk through of kernel code for a given
operator is applied to analyze the liveness of each tensor used within it.  The
liveness of each tensor can then be used to create an interference graph of
tensors where tensors on an interference graph have overlapping lifetimes.
This graph can then be used to apply a graph coloring strategy to cluster
tensors with non-overlapping lifetimes onto the same partition of the SPM,
while overlapping lifetimes will be allocated separate partitions. The paper
proved that such an automated solution reached close to or exceeded hand
written assembly kernels and reach on average a 90\% speed up over
non-optimized kernel code.  While this approach could have been generalized to
the overall DNN case from the kernel case, this work applies a different
approach similar to the work presented by Verma et al. 2004 using an ILP model.
However, Li et al. demonstrate that a greedy graph coloring approach can reach
close to optimal performance across a multitude of operators.

Verma et al. \cite{verma} introduce an ILP approach to the same register
allocation problem for single dimensional arrays as the work Li et al. extend
for DNNs. Verma et al.  reformulate the graph colorign problem into an ILP form
where constraints are added to ensure that tensors with overlapping lifetimes
are not allocated the same partition. The objective function is created to
maximize the number of data transfers saved. By using an ILP model over a
greedy algorithm, an optimal solution can be found. However, again, this
approach is not trivial to apply towards large variable tensors in DL workloads
due to assumptions the model takes of fixed size psuedo-registers and single
dimensional arrays.

% many core architecture
The problem of using more than one SPM is an issue that has mostly been ignored
until the work of Tao et  al. 2021 where they solve for an optimal data size
granularity to transfer between scratchpads and main memory to maximize performance
using a linear programming model for multicore architectures. The paper achieves
this via compiler-directed SPM data transfer model to formulate ann allocation
scheme in a heterogeneous many-core architecture and then use the same allocations
to determine the optimal data transfer granularity to maximize performance.

% on sram
