% Chapter Template

\chapter{OnSRAM and Proposed approach} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}
% do we need to talk about all the new accels that come out and why they are needed to run DL workloads instead of CPUs?

% TODO: talk about how this thing is a compiler extension that can be plugged
% into frameworks and we like this since it isn't re-optimizing and fighting
% for the same intrano de optimizations every other framework is doing and
% doesn't require reimagingin how to make a computation graph or IR for the
% millionth time. So this is attractive because its an actual unsolved problem
% compared to the rest

OnSRAM introduces the notion of two types of runtime performance optimizations
that can be done on graph based compilers: intra-node and inter-node
optimizations \cite{onsram}. Intra node optimizations are those that are
focused on optimizing the operation kernel that the node specifies. This means
tiling in favorable sizes and across specific dimensions, loop ordering, and
DMA pipelining between tiles \cite{aladdin}.

Inter node optimizations are optimizations concerning the overall structure and
ordering of the nodes in the graph and their connections. This includes
operator fusion and node reordering\cite{onsram}.
OnSRAM focuses on internode optimizations, specifically in the domain of
scratchpad memory management techniques for DLAs. 

Existing frameworks have neglected to focus on inter-node memory reuse
optimizations in favor of intra-node optimizations \cite{TVM} \cite{deeptools}
\cite{tensorflow}. Many mentioned frameworks are focused on creating an overall
compiler framework that increases the usability and hardware abstractions for
machine learning engineers by enabling hardware agnostic support and
optimizations. As a consequence, each framework has taken a different approach
or extended the capabilities of other existing frameworks to achieve the same
goal. In comparison, OnSRAM is a compiler extension made to focus on an
algorithm specific to just one type of memory optimization.

% TODO: its obvious but explain the amortization comment
% TODO: create figure of reuse graph
The motivation of such optimizations come from the minimization of memory
transfers which speed up inference runs by up to 5.17x \cite{onsram}. OnSRAM
exploits the repeating usage of outputs in a graph and identifies the outputs
that can pinned to minimize memory transfers which decrease inference time and
decrease energy costs. The ultimate performance gained from pinning outputs
comes from multiple inference runs where the cost of mapping pinnable tensors
are amortized by the time saved through iterative inference runs of the same
model.

Such inter-node memory management techniques occur due to the presence of
inherent repeating patterns of operations to support deep learning
architectures, including matrix multiplication and convolution operations.


%-----------------------------------
%	SUBSECTION 1
%-----------------------------------

\subsection{Static Graph Execution}

OnSRAM-Static, the static DNN Graph SPM manager is described as follows. A
graph of operations that represent a DNN is passed to OnSRAM-Static as an
input. The vertexes of the graph are the operations to be executed, e.g matrix
multiply, and the edges represent input and output tensors.  All input and
output data tensors of each node is analyzed for the start time, end time,
number of reuses, distance between reuses, and size of the tensor. These
properties are then used to determine the optmial pinning scheme to minimize
memory transfers. Each of these properties are considered with weights on how
important each property is and the tensors are psuedo-sorted in cost order;
they are psuedo-sorted since tensors that don't overlap in their lifetimes need
not be sorted relative to each other. These sorted tensors are then considered
in a greedy fashion where tensors of the highest cost are considered for
pinning first and the rest are considered to be pinned if and only if it can be
accommodated in spite of the higher cost tensor being pinned. Tensors are only
considered for pinning if they can be pinned for their entire duration of their
lifetime and can fit on the SPM with the other inputs and outputs for every
operation they are pinned for.

OnSRAM-Static only applies to input and output tensors, not weights. This is
due to the fact that weights are single use objects that are only needed for
the layers they are used in. There exists no reuse opportunity unlike output
tensors since all layers contain independently different weights. Thus, weights
are not considered for pinning for both OnSRAM-Static and our extension work as
well.

Another aspect that OnSRAM does not consider is applicability towards training.
The assumptions of the problem change when considering training and
significantly increase the difficulty and search space. This requires 
separate compiler extension implementations to be designed into separate
portions of the framework. Further, batched training, partitioned accelerators,
distributed systems, and large reuse distances due to backward passes and
batched data \cite{onsram} bring significant design challenges that are not present in
inference. Thus we have also chosen not to extend this work in this regard as
well.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Simulation and Architecture}
% how important is this question? should we just be saying what we used

Differences in hardware and simulators don't affect the relative performance of
a non-pinning compiler compared to a pinning compiler, however there are some
important environment variables that will consequently affect repeatability and
benchmark performance. Most notably, the simulator and hardware architecture in
use.

OnSRAM uses a cycle accurate simulator to model and assess the performance of
their algorithm \cite{onsram}.  We have opted to use the SMAUG\cite{smaug}
framework that depends on the Aladdin\cite{aladdin} system to model our
accelerator and the gem5 simulator\cite{gem5} to implement our experiments.
While gem5 is not cycle accurate, it still advertises a close to cycle accurate
performance metrics and creates a relative base of comparison between base
models and our optimized model.

% TODO: explain smaug more. like what it is and stuff and like other stuff from
% the paper

Intra-node optimizations that both simulators do but may not do the same way
that may affect performance comparisons: loop tiling, loop ordering, unrolling,
and pipelined DMA operations for maximizing intra-node reuse.

% architectural differences
We use the default SMAUG provided accelerator inspired by NVDLA\cite{smaug}.
The accelerator contains eight PEs, multiply accumalate (MACC) arrays, and
contains 3 scratchpad memories\cite{smaug}. Two scratchpads are configured to
be used as inputs and one for output.

% do we need to specify onsram architecture?

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Extensions}

%-----------------------------------
%	SUBSECTION 1
%-----------------------------------

\subsection{OnSRAM Limitations}

While OnSRAM has contributed a novel SPM management scheme that no framework
has explored \cite{onsram}, there are limitations that don't allow it to
generalize in a hardware agnostic scenario. This is due to its assumption that
the accelerator contains only one shared scratchpad for input and output
tensors. A consequence of this is that the greedy algorithm proposed to map
possible pin candidates is non-trivial to apply to an accelerator with more
than one scratchpad. Further, because of the heuristic approach, OnSRAM is not
able to achieve the optimal memory mappings with respect to minimizing memory
transfer.

%-----------------------------------
%	SUBSECTION 2
%-----------------------------------

\subsection{Motivating Example}

% explain how onsram does the thing

Consider the computational graph of in figure 1. The graph clearly shows that
node 0's output will be reused as inputs to node 1 and 2. In a single SPM case,
we can see how the tensors would be mapped for each timestep. Clearly, the leftover
space of the SPM is what we can use as leftover pinning space between operations.
However, in the case of a multi-SPM architecture, this is not the case as tensors
cannot be split over different SPMs. The other main point is that, to reach an optimal
or close to optimal mapping of pinned tensors, the strategy must consider which SPM
to pin it on if multiple SPMs have enough space. Once an SPM has been pinned, the
mapping of other inputs, outputs, and future pins will then be affect the availble
space and ability to maximize the nubmer of pinned tensors. The choice of where to
map each input, output, and pinned tensor per operation creates a combinatorial
explosion in the search space. A toy model is illustrated in figure 3.

Utilizing a greedy approach in the context of multiple SPMs, i.e., initially
pinning an item and remapping the inputs and outputs accordingly, essentially
results in a dilemma comparable to the bin-packing problem. This process
involves eviction when the pinned item cannot accommodate all other items, or
when the output of the present operation is of higher importance. The initial
decision of selecting an SPM to pin an item, which depends on the primary
remapping of inputs and outputs around a pinned tensor, can considerably
influence the pinnability of future items. This might, in certain instances,
lead to the unpinned state of items immediately prior to their reuse.

In such an approach, the system may not adequately identify opportunities for
reuse, leading to unpredictable decision-making and near-suboptimal results.
This limitation persists, albeit in a reduced form, even when the size of the
SPM is increased. Therefore, to devise an algorithm that meticulously analyzes
possible decisions and maps tensors in a manner that effectively encourages
reuse, one could implement techniques such as graph coloring or ILP, which are
similar to the methods used in solving the pseudo-register SPM register
allocation problem.

Choosing an optimal pinning strategy to minimize the amount of data transfers
within this search space is the goal of this work.

\subsection{Proposed Approach}
% we do everything in the 3 spm case
% goal is to mitigate data transfers between SPMs and main memory
% In order to analyze the possible mapping
In order to analyze the search space and create a pin mapping for all tensors
such that the number of data transfers between SPMs and main memory on an
inter-node graph level are minimized, we propose an ILP model to create an
optimal strategy for a multi-scratchpad architecture.

To do this we first take an optimized computational graph and map it as a final
schedule of operators that will be ran. Using this schedule of operators, we
then aggregate all tensors that are used in the DNN. Each input and output
tensor is annotated with the operator from which it was created from and all
operators that use it as an input from creation until the last use of the
tensor. All tensor sizes, number of SPMs, and the size of the SPMs are gathered
as well. Using this information an initial naive mapping scheme can be created
where all inputs and outputs are mapped to a designated input SPM and output
SPM. All outputs are assumed to be saved back to memory and reloaded when
needed. We then use the initial mapping as an input into the ILP solver to realize the
optimal strategy.
