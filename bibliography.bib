@article{TVM,
	author       = {Tianqi Chen and
		 Thierry Moreau and
		 Ziheng Jiang and
		 Haichen Shen and
		 Eddie Q. Yan and
		 Leyuan Wang and
		 Yuwei Hu and
		 Luis Ceze and
		 Carlos Guestrin and
		 Arvind Krishnamurthy},
	title        = {{TVM:} End-to-End Optimization Stack for Deep Learning},
	journal      = {CoRR},
	volume       = {abs/1802.04799},
	year         = {2018},
	url          = {http://arxiv.org/abs/1802.04799},
	eprinttype    = {arXiv},
	eprint       = {1802.04799},
	timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1802-04799.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}



@article{IBM,
	Abstract = {Operating a laser diode in an extended cavity which provides frequency-selective feedback is a very effective method of reducing the laser's linewidth and improving its tunability. We have developed an extremely simple laser of this type, built from inexpensive commercial components with only a few minor modifications. A 780~nm laser built to this design has an output power of 80~mW, a linewidth of 350~kHz, and it has been continuously locked to a Doppler-free rubidium transition for several days.},
	Author = {A. S. Arnold and J. S. Wilson and M. G. Boshier and J. Smith},
	Journal = {Review of Scientific Instruments},
	Month = {3},
	Number = {3},
	Numpages = {4},
	Pages = {1236--1239},
	Title = {A Simple Extended-Cavity Diode Laser},
	Volume = {69},
	Url = {http://link.aip.org/link/?RSI/69/1236/1},
	Year = {1998}}

@article{onsram,
	author = {Pal, Subhankar and Venkataramani, Swagath and Srinivasan, Viji and Gopalakrishnan, Kailash},
	title = {OnSRAM: Efficient Inter-Node On-Chip Scratchpad Management in Deep Learning Accelerators},
	year = {2022},
	issue_date = {November 2022},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {21},
	number = {6},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3530909},
	doi = {10.1145/3530909},
	abstract = {Hardware acceleration of Artificial Intelligence (AI) workloads has gained widespread popularity with its potential to deliver unprecedented performance and efficiency. An important challenge remains in how AI accelerators are programmed to sustain high utilization without impacting end-user productivity. Prior software optimizations start with an input graph and focus on node-level optimizations, viz. dataflows and hierarchical tiling, and graph-level optimizations such as operation fusion. However, little effort has been devoted to inter-node on-chip scratchpad memory (SPM) management in Deep Learning (DL) accelerators, whose significance is bolstered by the recent trends in complex network topologies and the emergence of eager execution in DL frameworks.We characterize and show that there exists up to a 5.2\texttimes{} performance gap in DL inference to be bridged using SPM management and propose OnSRAM, a novel SPM management framework integrated with the compiler runtime of a DL accelerator. We develop two variants, viz. &nbsp;OnSRAM-Static, which works on static graphs to identify data structures that can be lucratively held on-chip based on their size, liveness and significance, and OnSRAM-Eager, which targets an eager execution model (no graph) and uses a history-based speculative scheme to hold/discard data structures. We integrate OnSRAM &nbsp;with TensorFlow and analyze it on multiple accelerator configurations. Across a suite of 12 images, objects, and language networks, on a 3&nbsp;TFLOP system with a 2&nbsp;MB SPM and 32&nbsp;GBps external memory bandwidth, OnSRAM-Static &nbsp;and OnSRAM-Eager &nbsp;achieve 1.02–4.8\texttimes{} and 1.02–3.1\texttimes{} reduction in inference latency (batch size of 1), over a baseline with no SPM management. In terms of energy savings, we observe average reductions of 1.51\texttimes{} (up to 4.1\texttimes{}) and 1.23\texttimes{} (up to 2.9\texttimes{}) for the static and eager execution scenarios, respectively.},
	journal = {ACM Trans. Embed. Comput. Syst.},
	month = {oct},
	articleno = {86},
	numpages = {29},
	keywords = {Artificial intelligence, neural networks, compiler, hardware acceleration, memory management}
}




@article{aladdin,
	author       = {Tianqi Chen and
		 Thierry Moreau and
		 Ziheng Jiang and
		 Haichen Shen and
		 Eddie Q. Yan and
		 Leyuan Wang and
		 Yuwei Hu and
		 Luis Ceze and
		 Carlos Guestrin and
		 Arvind Krishnamurthy},
	title        = {{TVM:} End-to-End Optimization Stack for Deep Learning},
	journal      = {CoRR},
	volume       = {abs/1802.04799},
	year         = {2018},
	url          = {http://arxiv.org/abs/1802.04799},
	eprinttype    = {arXiv},
	eprint       = {1802.04799},
	timestamp    = {Sat, 17 Dec 2022 01:15:27 +0100},
	biburl       = {https://dblp.org/rec/journals/corr/abs-1802-04799.bib},
	bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{tensorflow,
	title={TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}, 
	author={Martín Abadi and Ashish Agarwal and Paul Barham and Eugene Brevdo and Zhifeng Chen and Craig Citro and Greg S. Corrado and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Ian Goodfellow and Andrew Harp and Geoffrey Irving and Michael Isard and Yangqing Jia and Rafal Jozefowicz and Lukasz Kaiser and Manjunath Kudlur and Josh Levenberg and Dan Mane and Rajat Monga and Sherry Moore and Derek Murray and Chris Olah and Mike Schuster and Jonathon Shlens and Benoit Steiner and Ilya Sutskever and Kunal Talwar and Paul Tucker and Vincent Vanhoucke and Vijay Vasudevan and Fernanda Viegas and Oriol Vinyals and Pete Warden and Martin Wattenberg and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year={2016},
	eprint={1603.04467},
	archivePrefix={arXiv},
	primaryClass={cs.DC}
}


@misc{gem5,
	title={The gem5 Simulator: Version 20.0+}, 
	author={Jason Lowe-Power and Abdul Mutaal Ahmad and Ayaz Akram and Mohammad Alian and Rico Amslinger and Matteo Andreozzi and Adrià Armejach and Nils Asmussen and Brad Beckmann and Srikant Bharadwaj and Gabe Black and Gedare Bloom and Bobby R. Bruce and Daniel Rodrigues Carvalho and Jeronimo Castrillon and Lizhong Chen and Nicolas Derumigny and Stephan Diestelhorst and Wendy Elsasser and Carlos Escuin and Marjan Fariborz and Amin Farmahini-Farahani and Pouya Fotouhi and Ryan Gambord and Jayneel Gandhi and Dibakar Gope and Thomas Grass and Anthony Gutierrez and Bagus Hanindhito and Andreas Hansson and Swapnil Haria and Austin Harris and Timothy Hayes and Adrian Herrera and Matthew Horsnell and Syed Ali Raza Jafri and Radhika Jagtap and Hanhwi Jang and Reiley Jeyapaul and Timothy M. Jones and Matthias Jung and Subash Kannoth and Hamidreza Khaleghzadeh and Yuetsu Kodama and Tushar Krishna and Tommaso Marinelli and Christian Menard and Andrea Mondelli and Miquel Moreto and Tiago Mück and Omar Naji and Krishnendra Nathella and Hoa Nguyen and Nikos Nikoleris and Lena E. Olson and Marc Orr and Binh Pham and Pablo Prieto and Trivikram Reddy and Alec Roelke and Mahyar Samani and Andreas Sandberg and Javier Setoain and Boris Shingarov and Matthew D. Sinclair and Tuan Ta and Rahul Thakur and Giacomo Travaglini and Michael Upton and Nilay Vaish and Ilias Vougioukas and William Wang and Zhengrong Wang and Norbert Wehn and Christian Weis and David A. Wood and Hongil Yoon and Éder F. Zulian},
	year={2020},
	eprint={2007.03152},
	archivePrefix={arXiv},
	primaryClass={cs.AR}
}

@misc{smaug,
      title={SMAUG: End-to-End Full-Stack Simulation Infrastructure for Deep Learning Workloads}, 
      author={Sam Likun Xi and Yuan Yao and Kshitij Bhardwaj and Paul Whatmough and Gu-Yeon Wei and David Brooks},
      year={2019},
      eprint={1912.04481},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deeptools,

	author={Venkataramani, Swagath and Choi, Jungwook and Srinivasan, Vijayalakshmi and Wang, Wei and Zhang, Jintao and Schaal, Marcel and Serrano, Mauricio J. and Ishizaki, Kazuaki and Inoue, Hiroshi and Ogawa, Eri and Ohara, Moriyoshi and Chang, Leland and Gopalakrishnan, Kailash},

	journal={IEEE Micro}, 

	title={DeepTools: Compiler and Execution Runtime Extensions for RaPiD AI Accelerator}, 

	year={2019},

	volume={39},

	number={5},

	pages={102-111},

	doi={10.1109/MM.2019.2931584}}

@misc{DLVM, title={DLVM: A modern compiler infrastructure for deep
	learning systems}, author={Richard Wei and Lane Schwartz and Vikram
	Adve}, year={2018}, eprint={1711.03016}, archivePrefix={arXiv},
	primaryClass={cs.PL} }

@misc{nGraph,
	title={Intel nGraph: An Intermediate Representation, Compiler, and Executor for Deep Learning}, 
	author={Scott Cyphers and Arjun K. Bansal and Anahita Bhiwandiwalla and Jayaram Bobba and Matthew Brookhart and Avijit Chakraborty and Will Constable and Christian Convey and Leona Cook and Omar Kanawi and Robert Kimball and Jason Knight and Nikolay Korovaiko and Varun Kumar and Yixing Lao and Christopher R. Lishka and Jaikrishnan Menon and Jennifer Myers and Sandeep Aswath Narayana and Adam Procter and Tristan J. Webb},
	year={2018},
	eprint={1801.08058},
	archivePrefix={arXiv},
	primaryClass={cs.DC}
}

@misc{onnx,
	title={Compiling ONNX Neural Network Models Using MLIR}, 
	author={Tian Jin and Gheorghe-Teodor Bercea and Tung D. Le and Tong Chen and Gong Su and Haruki Imai and Yasushi Negishi and Anh Leu and Kevin O'Brien and Kiyokuni Kawachiya and Alexandre E. Eichenberger},
	year={2020},
	eprint={2008.08272},
	archivePrefix={arXiv},
	primaryClass={cs.PL}
}

@techreport{cntk,
author = {Yu, Dong and Eversole, Adam and Seltzer, Mike and Yao, Kaisheng and Kuchaiev, Oleksii and Zhang, Yu and Seide, Frank and Huang, Zhiheng and Guenter, Brian and Wang, Huaming and Droppo, Jasha and Zweig, Geoffrey and Rossbach, Chris and Gao, Jie and Stolcke, Andreas and Currey, Jon and Slaney, Malcolm and Chen, Guoguo and Agarwal, Amit and Basoglu, Chris and Padmilac, Marko and Kamenev, Alexey and Ivanov, Vladimir and Cypher, Scott and Parthasarathi, Hari and Mitra, Bhaskar and Peng, Baolin and Huang, Xuedong},
title = {An Introduction to Computational Networks and the Computational Network Toolkit},
year = {2014},
month = {October},
abstract = {We introduce computational network (CN), a unified framework for describing arbitrary learning machines, such as deep neural networks (DNNs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), long short term memory (LSTM), logistic regression, and maximum entropy model, that can be illustrated as a series of computational steps. A CN is a directed graph in which each leaf node represents an input value or a parameter and each non-leaf node represents a matrix operation upon its children. We describe algorithms to carry out forward computation and gradient calculation in CN and introduce most popular computation node types used in a typical CN.

We further introduce the computational network toolkit (CNTK), an implementation of CN that supports both GPU and CPU. We describe the architecture and the key components of the CNTK, the command line options to use CNTK, and the network definition and model editing language, and provide sample setups for acoustic model, language model, and spoken language understanding. We also describe the Argon speech recognition decoder as an example to integrate with CNTK.},
publisher = {Microsoft Research},
url = {https://www.microsoft.com/en-us/research/publication/an-introduction-to-computational-networks-and-the-computational-network-toolkit/},
number = {MSR-TR-2014-112},
}

@INPROCEEDINGS{memoryColoring,
  author={Lian Li and Lin Gao and Jingling Xue},
  booktitle={14th International Conference on Parallel Architectures and
Compilation Techniques (PACT'05)}, 
  title={Memory coloring: a compiler approach for scratchpad memory
management}, 
  year={2005},
  volume={},
  number={},
  pages={329-338},
  doi={10.1109/PACT.2005.27}}

@article{graphColoring,
	author = {Li, Lian and Feng, Hui and Xue, Jingling},
	title = {Compiler-Directed Scratchpad Memory Management via Graph Coloring},
	year = {2009},
	issue_date = {September 2009},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {6},
	number = {3},
	issn = {1544-3566},
	url = {https://doi.org/10.1145/1582710.1582711},
	doi = {10.1145/1582710.1582711},
	abstract = {Scratchpad memory (SPM), a fast on-chip SRAM managed by software, is widely used in embedded systems. This article introduces a general-purpose compiler approach, called memory coloring, to assign static data aggregates, such as arrays and structs, in a program to an SPM. The novelty of this approach lies in partitioning the SPM into a pseudo--register file (with interchangeable and aliased registers), splitting the live ranges of data aggregates to create potential data transfer statements between SPM and off-chip memory, and finally, adapting an existing graph coloring algorithm for register allocation to assign the data aggregates to the pseudo--register file. Our experimental results using a set of 10 C benchmarks from MediaBench and MiBench show that our methodology is capable of managing SPMs efficiently and effectively for large embedded applications. In addition, our SPM allocator can obtain close to optimal solutions when evaluated and compared against an existing heuristics-based SPM allocator and an ILP-based SPM allocator.},
	journal = {ACM Trans. Archit. Code Optim.},
	month = {oct},
	articleno = {9},
	numpages = {17},
	keywords = {software-managed cache, memory coloring, Scratchpad memory, graph coloring, register coalescing, live range splitting, memory allocation}
}

      

@article{toplib,
author = {Li, Jiansong and Cao, Wei and Dong, Xiao and Li, Guangli and Wang, Xueying and Zhao, Peng and Liu, Lei and Feng, Xiaobing},
title = {Compiler-Assisted Operator Template Library for DNN Accelerators},
year = {2021},
issue_date = {Oct 2021},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {49},
number = {5},
issn = {0885-7458},
url = {https://doi.org/10.1007/s10766-021-00701-6},
doi = {10.1007/s10766-021-00701-6},
abstract = {Despite many dedicated accelerators are gaining popularity for their performance and energy efficiency in the deep neural network (DNN) domain, high-level programming support for these accelerators remains thin. In contrast to existing researches targeting the whole DNNs, we choose to dive into details and review this problem from a finer-grained level, operators. Due to performance concerns, operator programmers may have to take hand-written assembly as their first choice, which is error-prone and involves many programming chores. To alleviate this problem, we propose TOpLib, a compiler-assisted template library. By providing a unified user-view abstraction, TOpLib allows programmers to express computational kernels with high-level tensor primitives, which will be automatically lowered into low-level intrinsic primitives via expression templates. Moreover, considering memory management is performance-critical and the optimization strategy of expression template is limited to enumeration based rewriting rules, we implement TOpLib with a compiler-assisted approach. We address the memory reuse challenges into the compiler, which allows TOpLib to make full use of on-chip buffers and result in better performance. Experiments over 55 typical DNN operators demonstrate that TOpLib can generate scalable code with performance faster than or on par with hand-written assembly versions.},
journal = {Int. J. Parallel Program.},
month = {oct},
pages = {628–645},
numpages = {18},
keywords = {Template Library, DNN Accelerators, Address Space Management}
}

@article{registerAllocation,
author = {Briggs, Preston and Cooper, Keith D. and Torczon, Linda},
title = {Improvements to Graph Coloring Register Allocation},
year = {1994},
issue_date = {May 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/177492.177575},
doi = {10.1145/177492.177575},
abstract = {We describe two improvements to Chaitin-style graph coloring register allocators. The first, optimistic coloring, uses a stronger heuristic to find a k-coloring for the interference graph. The second extends Chaitin's treatment of rematerialization to handle a larger class of values. These techniques are complementary. Optimistic coloring decreases the number of procedures that require spill code and reduces the amount of spill code when spilling is unavoidable. Rematerialization lowers the cost of spilling some values. This paper describes both of the techniques and our experience building and using register allocators that incorporate them. It provides a detailed description of optimistic coloring and rematerialization. It presents experimental data to show the performance of several versions of the register allocator on a suite of FORTRAN programs. It discusses several insights that we discovered only after repeated implementation of these allocators.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {may},
pages = {428–455},
numpages = {28},
keywords = {register allocation, code generation, graph coloring}
}

@INPROCEEDINGS{verma,

  author={Verma, M. and Wehmeyer, L. and Marweclel, P.},

  booktitle={International Conference on Hardware/Software Codesign and System Synthesis, 2004. CODES + ISSS 2004.}, 

  title={Dynamic overlay of scratchpad memory for energy minimization}, 

  year={2004},

  volume={},

  number={},

  pages={104-109},

  doi={}}

@article{manyCore,
author = {Tao, Xiaohan and Pang, Jianmin and Xu, Jinlong and Zhu, Yu},
year = {2021},
month = {12},
pages = {},
title = {Compiler-directed scratchpad memory data transfer optimization for multithreaded applications on a heterogeneous many-core architecture},
volume = {77},
journal = {The Journal of Supercomputing},
doi = {10.1007/s11227-021-03853-x}
}

@misc{nvdla,
	title = {NVDLA Home Page},
	howpublished = {\url{https://nvdla.org/#}},
	note = {Accessed: 2022-11-05}
}

@misc{tensorRT,
	title = {NVIDIA TensorRT Docs Hub},
	howpublished = {\url{https://docs.nvidia.com/tensorrt/index.html}},
	note = {Accessed: 2022-11-05}
}

@article{nnp,
	title={Spring Hill (NNP-I 1000) Intel’s Data Center Inference Chip},
	author={Ofri Wechsler and Michael Behar and Bharat Daga},
	journal={2019 IEEE Hot Chips 31 Symposium (HCS)},
	year={2019},
	pages={1-12}
}
